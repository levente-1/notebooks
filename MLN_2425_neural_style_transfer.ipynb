{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvRgSoPYS+CiUw2iv3sZD/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/levente-1/notebooks/blob/main/MLN_2425_neural_style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1WylQLZX3zEO2lmKY9m8-FbfliJcXFp8L)"
      ],
      "metadata": {
        "id": "caP99QG1ob4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning in Neuroscience\n",
        "\n",
        "# Practical 7 - Neural Style Transfer\n",
        "\n",
        "Neural Style Transfer (NST) uses a previously trained convolutional network, and builds on top of that. The idea of using a network trained on a different task and applying it to a new task is called transfer learning.\n",
        "\n",
        "You will be using the the epynomously named VGG network from the [original NST paper](https://arxiv.org/abs/1508.06576) published by the Visual Geometry Group at University of Oxford in 2014. Specifically, you'll use VGG-19, a 19-layer version of the VGG network. This model has already been trained on the very large ImageNet database, and has learned to recognize a variety of low level features (at the shallower layers) and high level features (at the deeper layers).\n",
        "\n",
        "As explained in the lecture, NST involves the optimisation of a generated image based on a style target and a content target. Throughout this notebook, we will refer to the generated image as G, the content image as C, and the style image as S."
      ],
      "metadata": {
        "id": "w9_m4aL7oe8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Cloning a GitHub repository and downloading packages\n",
        "\n",
        "As part of this practical, we will use a [PyTorch implementation](https://github.com/gordicaleksa/pytorch-neural-style-transfer) of NST, written by Aleksa Gordic.\n",
        "\n",
        "To begin, we will need to:\n",
        "\n",
        "1.   Mount our drive to allow this notebook to access Google Drive\n",
        "2.   Create a new folder\n",
        "3.   Clone the repository into our newly created folder\n",
        "\n"
      ],
      "metadata": {
        "id": "giky_zY9o7Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WigphepnodYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "Tdt4_j8iqgq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive"
      ],
      "metadata": {
        "id": "jV5_PML1qlsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir Practical7\n",
        "cd Practical 7"
      ],
      "metadata": {
        "id": "whTPqDmzqyRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the GitHub repository containing all of the code we will be using\n",
        "! git clone https://github.com/gordicaleksa/pytorch-neural-style-transfer"
      ],
      "metadata": {
        "id": "1GzkB2Gjric1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd pytorch-neural-style-transfer"
      ],
      "metadata": {
        "id": "-s5fJUxlrslx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, import all packages required\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "from torch.optim import Adam, LBFGS\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import os\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "metadata": {
        "id": "mBfoDqjPr3qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Content Reconstruction\n",
        "\n",
        "One goal you should aim for when performing NST is for the content in generated image G to match the content of image C. To do so, you'll need an understanding of the features captured by <b> shallow versus deep layers </b>.\n",
        "\n",
        "We will begin by running a script that starts from random noise, and reconstructs image C by minimising the content loss only (MSE between features obtained from image G and features obtained from image C; see lecture). Before running our code, let's quickly take a look at our target image C:\n"
      ],
      "metadata": {
        "id": "sX_m5rtAkJej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = 'data/content-images/lion.jpg'\n",
        "img = mpimg.imread(img_path)\n",
        "imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J-c6y-f6i6dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run the code in the cell below, which calls on the <i> \"reconstruct_image_from_representation.py\" </i> script in your repository that you cloned from GitHub:"
      ],
      "metadata": {
        "id": "FXfmWDdpjLSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python reconstruct_image_from_representation.py"
      ],
      "metadata": {
        "id": "Y0Uri-xxORdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4-1-1'></a>\n",
        "#### 2.1 Feature maps from image C and reconstructed image\n",
        "\n",
        "The outputs of the script above will be placed in:\n",
        "\n",
        "\n",
        "- <i> data/output-images/c_reconstruction_lbfgs/lion </i>\n",
        "\n",
        "\n",
        "Here, we will see two types of image files:\n",
        "\n",
        "\n",
        "1.   Gradual reconstruction images from noise to content\n",
        "2.   Feature maps in a given layer (in this case layer 2 of VGG-19)\n",
        "\n",
        "To visualise the feature maps we captured from our image, run the code below:\n",
        "\n"
      ],
      "metadata": {
        "id": "OLItRiqXjU1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_number = 42 # Can set this to anywhere between 0-127\n",
        "img_path = 'data/output-images/c_reconstruction_lbfgs/lion/fm_vgg19_relu2_1_'+str(filter_number).zfill(4)+'.jpg'\n",
        "img = mpimg.imread(img_path)\n",
        "imgplot = plt.imshow(img, cmap='Accent')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dZ0NLjGIxvz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualise the reconstructed image, we can use the cell below to create a GIF (going from pure noise to the final content image) or the cell afterwards to view individual reconstructions:"
      ],
      "metadata": {
        "id": "MqLmgrNAmJrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running this cell may take 1-2 mins\n",
        "from PIL import Image\n",
        "outDir = 'data/output-images/c_reconstruction_lbfgs/lion'\n",
        "images_lion = os.listdir(outDir)\n",
        "for i in images_lion[:]:\n",
        "  if 'vgg' in i or 'jpg' not in i:\n",
        "    images_lion.remove(i)\n",
        "images_final = []\n",
        "for i in images_lion:\n",
        "  im = Image.open(os.path.join(outDir, i))\n",
        "  images_final.append(im)\n",
        "\n",
        "images_final[0].save(os.path.join(outDir, 'Lion.gif'), save_all=True, append_images=images_final[1:], duration=40, loop=0)"
      ],
      "metadata": {
        "id": "N9FoDcWZahtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: you may need to change the number of the image file\n",
        "img = mpimg.imread('data/output-images/c_reconstruction_lbfgs/lion/0358.jpg')\n",
        "imgplot = plt.imshow(img, cmap='Accent')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EbItToZ_zIpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4-1-1'></a>\n",
        "#### 2.2 Feature maps from different layers\n",
        "\n",
        "The script, as written by default, uses layer 2 from VGG to obtain feature maps. As we saw above, this results in features that capture fine details.\n",
        "\n",
        "To see what feature maps are obtained using deeper layers, we need to modify our script. To do this, complete the following steps:\n",
        "\n",
        "1.   Double-click on <i> \"models/definitions/vgg_nets.py\" </i> to open an editor\n",
        "2.   On line 176, change <i> self.content_feature_maps_index </i> from 1 to 4. This will allow us to extract features from layer 5 instead of layer 2\n",
        "3.   To make sure a new directory is created when rerunning <i> \"reconstruct_image_from_representation.py\" </i>, double click on this script and change line 46 by altering 'dump_path' with an added suffix of your choice\n",
        "\n",
        "Once all these steps are completed, we are now ready to reconstruct content from noise again by running the cell below:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5KjM4uaBxZHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python reconstruct_image_from_representation.py"
      ],
      "metadata": {
        "id": "24KOYzlOxcah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualise the feature maps obtained, this time from Layer 5 of VGG. Do you see any differences compared to what we saw from earlier layers?"
      ],
      "metadata": {
        "id": "z3F-aiSjqWiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_number = 11 # Can set this to anywhere between 0-511\n",
        "img_path = 'data/output-images/c_reconstruction_lbfgs2/lion/fm_vgg19_conv4_2_'+str(filter_number).zfill(4)+'.jpg'\n",
        "img = mpimg.imread(img_path)\n",
        "imgplot = plt.imshow(img, cmap='Accent')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LQIupw2mQU-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's visualise the reconstructed content image. How does this differ from the previous reconstructed image?"
      ],
      "metadata": {
        "id": "1zpQs8_Uqivs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You may need to change the number of the image file\n",
        "img = mpimg.imread('data/output-images/c_reconstruction_lbfgs2/lion/0358.jpg')\n",
        "imgplot = plt.imshow(img, cmap='Accent')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iXnxwf4ORG-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2'></a>\n",
        "## 3 - Style Reconstruction\n",
        "\n",
        "The second goal you should aim for when performing NST is for the style in the generated image G to match the style of image S. To do so, you'll need an understanding of <b> Gram matrices. </b>\n",
        "\n",
        "To viualise Gram matrices and see how they aid in conveying style, we will use the same script as above with some minor modifications. Once again, we will start from random noise, however this time we will gradually construct an image that matches the style of image S by minimisng the style loss only (MSE between Gram matrices from 5 different layers, obtained from both image G and image S; see lecture). Before running our code, let's quickly take a look at image S:\n"
      ],
      "metadata": {
        "id": "ELO-y3UNctAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = 'data/style-images/ben_giles.jpg'\n",
        "img = mpimg.imread(img_path)\n",
        "imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1DAeEUcFeFJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's make the following edits to our code:\n",
        "\n",
        "1.   Open <i> \"reconstruct_image_from_representation.py\" </i> to edit the file\n",
        "2.   Revert line 46 back to the original, such that no extra suffix is added to \"dump_path\"\n",
        "3.   Change the <i> should_reconstruct_content </i> argument in line 155 to False: this will ensure the style reconstruction is carried out instead of content reconstruction\n",
        "\n",
        "\n",
        "After completing these steps you are now ready to run the cell below:"
      ],
      "metadata": {
        "id": "MCWxhVcaeFlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python reconstruct_image_from_representation.py"
      ],
      "metadata": {
        "id": "zxo2WMnUct2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4-1-1'></a>\n",
        "#### 3.1 Gram matrices from different layers\n",
        "\n",
        "The outputs of the script above will be placed in:\n",
        "\n",
        "\n",
        "- <i> data/output-images/s_reconstruction_lbfgs/ben_giles </i> (given that Ben Giles style image was used).\n",
        "\n",
        "\n",
        "Here, we will see two types of image files:\n",
        "\n",
        "\n",
        "1.   Gradual construction of style images\n",
        "2.   Gram matrices from layers 1-5\n",
        "\n",
        "To visualise the Gram matrices we captured from image S, run the code below:\n"
      ],
      "metadata": {
        "id": "tNV0hdYCf0IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_number = 1 # Can set this to anywhere between 0-4\n",
        "layer_names = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'conv4_2']\n",
        "img_path = 'data/output-images/s_reconstruction_lbfgs/ben_giles/gram_vgg19_' + layer_names[filter_number] + '_' + str(filter_number).zfill(4) + '.jpg'\n",
        "img = mpimg.imread(img_path)\n",
        "imgplot = plt.imshow(img, cmap='Accent')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "0GzNP1ZEeJmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualise our style reconstruction, we can use the cell below to create a GIF (going from pure noise to the final style image) or the cell afterwards to view individual reconstructions."
      ],
      "metadata": {
        "id": "UvGlYQaPhlNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "outDir = 'data/output-images/s_reconstruction_lbfgs/ben_giles'\n",
        "images_ben_giles = os.listdir(outDir)\n",
        "for i in images_ben_giles[:]:\n",
        "  if 'gram' in i or 'jpg' not in i:\n",
        "    images_ben_giles.remove(i)\n",
        "images_final = []\n",
        "for i in images_ben_giles:\n",
        "  im = Image.open(os.path.join(outDir, i))\n",
        "  images_final.append(im)\n",
        "\n",
        "images_final[0].save(os.path.join(outDir, 'Ben_Giles.gif'), save_all=True, append_images=images_final[1:], duration=40, loop=0)"
      ],
      "metadata": {
        "id": "9bMNYpHIfcQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: you may need to change the number of the image file\n",
        "img = mpimg.imread('data/output-images/s_reconstruction_lbfgs/ben_giles/0373.jpg')\n",
        "imgplot = plt.imshow(img, cmap='Accent')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aaAZ_Hr1hxzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2'></a>\n",
        "## 4 - Neural Style Transfer\n",
        "\n",
        "Finally, we are ready to run <strong>Neural Style Transfer</strong>. For this, we start with image C instead of a random noise image, and gradually inject the style of image S by minimising the style loss. Importantly, we by keeping track of the content loss during this time and make that it stays low, ensuring that the new style we inject does not make our original image unrecognisable.\n",
        "\n",
        "To see this process in action, run the code in the cell below, which calls on the <i> \"neural_style_transfer.py\" </i> script in your repository"
      ],
      "metadata": {
        "id": "_g5n55s9YhjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python neural_style_transfer.py"
      ],
      "metadata": {
        "id": "XUVs4S5eYkOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4-1-1'></a>\n",
        "#### 4.1 Visualising the outputs\n",
        "\n",
        "The outputs of the script above will be placed in:\n",
        "\n",
        "- <i> data/output-images/combined_lion_ben_giles </i>\n",
        "\n",
        "\n",
        "The folder will contain an image from each training epoch, gradually transforming the generated image to be a combination of image C and image S. Use the cell below to create a GIF that captures this transformation, or the cell afterwards to view the final image:"
      ],
      "metadata": {
        "id": "9SkEZwTalr6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "outDir = 'data/output-images/combined_lion_ben_giles'\n",
        "images_ben_giles = os.listdir(outDir)\n",
        "for i in images_ben_giles[:]:\n",
        "  if 'vgg' in i or 'jpg' not in i:\n",
        "    images_ben_giles.remove(i)\n",
        "images_final = []\n",
        "for i in images_ben_giles:\n",
        "  im = Image.open(os.path.join(outDir, i))\n",
        "  images_final.append(im)\n",
        "\n",
        "images_final[0].save(os.path.join(outDir, 'Lion_Ben_Giles.gif'), save_all=True, append_images=images_final[1:], duration=40, loop=0)"
      ],
      "metadata": {
        "id": "PM8JR8NXf9ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: you may need to change the number of the image file\n",
        "img = mpimg.imread('data/output-images/combined_lion_ben_giles/1015.jpg')\n",
        "imgplot = plt.imshow(img, cmap='Accent')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YMF59T3h2PhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4-1-1'></a>\n",
        "#### 4.2 Experimenting\n",
        "\n",
        "Now it's your turn to experiment with the code. Try and see if changing the layers from VGG (from vgg.py) or  altering the weights of the style and content loss (in neural_style_transfer.py) has effects on the constructed imgae. If so, what happens when changing these arguments?\n",
        "\n",
        "Finally, try this out with different content and style images from the ones provided, or upload your own jpg images!"
      ],
      "metadata": {
        "id": "_UojclKf2d7r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rrr3AyFq3MKh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}